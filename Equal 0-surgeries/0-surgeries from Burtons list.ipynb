{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step we have classified all knots with at most 15 crossings that share the same 0-surgeries. \n",
    "\n",
    "Now we want to find actually the pair of knots (K,K') that share the same 0-surgery and realize the minimum complexity c(K)+c(K').\n",
    "\n",
    "In a previous step we have created a list for every prime knot K with at most 9 crossings that contains all prime knots with at most 19 crossings with the same Alexander polynomial and complexity at most 25. These are candidates that might share the same 0-surgery. We continue to work with this list.\n",
    "\n",
    "First we load the necessary code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:181: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:181: DeprecationWarning: invalid escape sequence \\d\n",
      "<ipython-input-1-cef6550c8024>:181: DeprecationWarning: invalid escape sequence \\d\n",
      "  match = re.search('(.*) : #\\d+$', name)\n"
     ]
    }
   ],
   "source": [
    "import snappy\n",
    "import regina\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def all_positive(manifold):\n",
    "    '''\n",
    "    Checks if the solution type of a triangulation is positive.\n",
    "    '''\n",
    "    return manifold.solution_type() == 'all tetrahedra positively oriented'\n",
    "\n",
    "def find_positive_triangulations(manifold,number=1,tries=100):\n",
    "    '''\n",
    "    Searches for one triangulation with a positive solution type.\n",
    "    (Or if number is set to a different value also for different such triangulations.)\n",
    "    '''\n",
    "    M = manifold.copy()\n",
    "    pos_triangulations=[]\n",
    "    for i in range(tries):\n",
    "        if all_positive(M):\n",
    "            pos_triangulations.append(M)\n",
    "            if len(pos_triangulations)==number:\n",
    "                return pos_triangulations\n",
    "            break\n",
    "        M.randomize()\n",
    "    for d in M.dual_curves(max_segments=500):\n",
    "        X = M.drill(d)\n",
    "        X = X.filled_triangulation()\n",
    "        X.dehn_fill((1,0),-1)\n",
    "        for i in range(tries):\n",
    "            if all_positive(X):\n",
    "                pos_triangulations.append(X)\n",
    "                if len(pos_triangulations)==number:\n",
    "                    return pos_triangulations\n",
    "                break\n",
    "            X.randomize()\n",
    "\n",
    "    # In the closed case, here is another trick.\n",
    "    if all(not c for c in M.cusp_info('is_complete')):\n",
    "        for i in range(tries):\n",
    "            # Drills out a random edge\n",
    "            X = M.__class__(M.filled_triangulation())\n",
    "            if all_positive(X):\n",
    "                pos_triangulations.append(X)\n",
    "                if len(pos_triangulations)==number:\n",
    "                    return pos_triangulations\n",
    "            break\n",
    "            M.randomize()\n",
    "    return pos_triangulations\n",
    "\n",
    "def better_volume(M,index=100,try_hard=False):\n",
    "    '''Computes the verified volume. Returns 0 if SnapPy could not do it.'''\n",
    "    count=0\n",
    "    while count<index:\n",
    "        try:\n",
    "            return M.volume(verified=True)\n",
    "        except:\n",
    "            M.randomize()\n",
    "            count=count+1\n",
    "    if try_hard==True:\n",
    "        pos_triang=find_positive_triangulations(M,number=1,tries=index)\n",
    "        for X in pos_triang:\n",
    "            vol=better_volume(X,index)\n",
    "            if vol!=0:\n",
    "                return vol\n",
    "    return 0\n",
    "\n",
    "def change_notation(dt_code):\n",
    "    \"\"\"\n",
    "    Changes Dowker-Thistlewait notation from alphabetical to numerical\n",
    "    Input:\n",
    "        dt_code (string): alphabetical DT notation\n",
    "    Return:\n",
    "        (string): numerical DT notation\n",
    "    \"\"\"\n",
    "    alpha = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    Alpha = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    result = []\n",
    "    for letter in dt_code:\n",
    "        if letter in alpha:\n",
    "            result.append(2* (alpha.index(letter) + 1))\n",
    "        elif letter in Alpha:\n",
    "            result.append(-2 * (Alpha.index(letter) + 1))\n",
    "        else:\n",
    "            print(dt_code)\n",
    "    return \"DT: \" + str([tuple(result)])\n",
    "\n",
    "def fill_triangulation(M):\n",
    "    '''\n",
    "    Fills all cusps but one.\n",
    "    '''\n",
    "    if M.num_cusps()==1:\n",
    "        return M\n",
    "    M=M.filled_triangulation([0])\n",
    "    M=fill_triangulation(M)\n",
    "    return M\n",
    "\n",
    "#### This is Dunfield's util.py from his exceptional census\n",
    "\n",
    "####  for a snappy manifold M descibed as a single filling of a cusp (so do filled_triangulation() as needed) \n",
    "####  the command regina_name(M) gives what regina identifies M as\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This file provides functions for working with Regina (with a little\n",
    "help from SnapPy) to:\n",
    "\n",
    "1. Give a standard name (\"identify\") manifolds, especially Seifert and\n",
    "   graph manifolds.\n",
    "\n",
    "2. Find essential tori.\n",
    "\n",
    "3. Try to compute the JSJ decomposition.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import regina\n",
    "import snappy\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "def appears_hyperbolic(M):\n",
    "    acceptable = ['all tetrahedra positively oriented',\n",
    "                  'contains negatively oriented tetrahedra']\n",
    "    return M.solution_type() in acceptable and M.volume() > 0\n",
    "\n",
    "def children(packet):\n",
    "    child = packet.firstChild()\n",
    "    while child:\n",
    "        yield child\n",
    "        child = child.nextSibling()\n",
    "\n",
    "def to_regina(data):\n",
    "    if hasattr(data, '_to_string'):\n",
    "        data = data._to_string()\n",
    "    if isinstance(data, str):\n",
    "        if data.find('(') > -1:\n",
    "            data = closed_isosigs(data)[0]\n",
    "        return regina.Triangulation3(data)\n",
    "    assert isinstance(data, regina.Triangulation3)\n",
    "    return data\n",
    "\n",
    "def extract_vector(surface):\n",
    "    \"\"\"\n",
    "    Extract the raw vector of the (almost) normal surface in Regina's\n",
    "    NS_STANDARD coordinate system.\n",
    "    \"\"\"\n",
    "    S = surface\n",
    "    T = S.triangulation()\n",
    "    n = T.countTetrahedra()\n",
    "    ans = []\n",
    "    for i in range(n):\n",
    "        for j in range(4):\n",
    "            ans.append(S.triangles(i, j))\n",
    "        for j in range(3):\n",
    "            ans.append(S.quads(i, j))\n",
    "    A = regina.NormalSurface(T, regina.NS_STANDARD, ans)\n",
    "    assert A.sameSurface(S)\n",
    "    return ans\n",
    "\n",
    "def haken_sum(S1, S2):\n",
    "    T = S1.triangulation()\n",
    "    assert S1.locallyCompatible(S2)\n",
    "    v1, v2 = extract_vector(S1), extract_vector(S2)\n",
    "    sum_vec = [x1 + x2 for x1, x2 in zip(v1, v2)]\n",
    "    A = regina.NormalSurface(T, regina.NS_STANDARD, sum_vec)\n",
    "    assert S1.locallyCompatible(A) and S2.locallyCompatible(A)\n",
    "    assert S1.eulerChar() + S2.eulerChar() == A.eulerChar()\n",
    "    return A\n",
    "\n",
    "\n",
    "def census_lookup(regina_tri):\n",
    "    \"\"\"\n",
    "    Should the input triangulation be in Regina's census, return the\n",
    "    name of the manifold, dropping the triangulation number.\n",
    "    \"\"\"\n",
    "    hits = regina.Census.lookup(regina_tri)\n",
    "    hit = hits.first()\n",
    "    if hit is not None:\n",
    "        name = hit.name()\n",
    "        match = re.search('(.*) : #\\d+$', name)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return match\n",
    "\n",
    "def standard_lookup(regina_tri):\n",
    "    match = regina.StandardTriangulation.isStandardTriangulation(regina_tri)\n",
    "    if match:\n",
    "        return match.manifold()\n",
    "\n",
    "def closed_isosigs(snappy_manifold, trys=20, max_tets=50):\n",
    "    \"\"\"\n",
    "    Generate a slew of 1-vertex triangulations of a closed manifold\n",
    "    using SnapPy.\n",
    "    \n",
    "    >>> M = snappy.Manifold('m004(1,2)')\n",
    "    >>> len(closed_isosigs(M, trys=5)) > 0\n",
    "    True\n",
    "    \"\"\"\n",
    "    M = snappy.Manifold(snappy_manifold)\n",
    "    assert M.cusp_info('complete?') == [False]\n",
    "    surgery_descriptions = [M.copy()]\n",
    "\n",
    "    try:\n",
    "        for curve in M.dual_curves():\n",
    "            N = M.drill(curve)\n",
    "            N.dehn_fill((1,0), 1)\n",
    "            surgery_descriptions.append(N.filled_triangulation([0]))\n",
    "    except snappy.SnapPeaFatalError:\n",
    "        pass\n",
    "\n",
    "    if len(surgery_descriptions) == 1:\n",
    "        # Try again, but unfill the cusp first to try to find more\n",
    "        # dual curves.\n",
    "        try:\n",
    "            filling = M.cusp_info(0).filling\n",
    "            N = M.copy()\n",
    "            N.dehn_fill((0, 0), 0)\n",
    "            N.randomize()\n",
    "            for curve in N.dual_curves():\n",
    "                D = N.drill(curve)\n",
    "                D.dehn_fill([filling, (1,0)])\n",
    "                surgery_descriptions.append(D.filled_triangulation([0]))\n",
    "        except snappy.SnapPeaFatalError:\n",
    "            pass\n",
    "\n",
    "    ans = set()\n",
    "    for N in surgery_descriptions:\n",
    "        for i in range(trys):\n",
    "            T = N.filled_triangulation()\n",
    "            if T._num_fake_cusps() == 1:\n",
    "                n = T.num_tetrahedra()\n",
    "                if n <= max_tets:\n",
    "                    ans.add((n, T.triangulation_isosig(decorated=False)))\n",
    "            N.randomize()\n",
    "\n",
    "    return [iso for n, iso in sorted(ans)]\n",
    "\n",
    "def best_match(matches):\n",
    "    \"\"\"\n",
    "    Prioritize the most concise description that Regina provides to\n",
    "    try to avoid things like the Seifert fibered space of a node being\n",
    "    a solid torus or having several nodes that can be condensed into a\n",
    "    single Seifert fibered piece.\n",
    "    \"\"\"\n",
    "    \n",
    "    def score(m):\n",
    "        if isinstance(m, regina.SFSpace):\n",
    "            s = 0\n",
    "        elif isinstance(m, regina.GraphLoop):\n",
    "            s = 1\n",
    "        elif isinstance(m, regina.GraphPair):\n",
    "            s = 2\n",
    "        elif isinstance(m, regina.GraphTriple):\n",
    "            s = 3\n",
    "        elif m is None:\n",
    "            s = 10000\n",
    "        else:\n",
    "            s = 4\n",
    "        return (s, str(m))\n",
    "    return min(matches, key=score)\n",
    "\n",
    "def identify_with_torus_boundary(regina_tri):\n",
    "    \"\"\"\n",
    "    Use the combined power of Regina and SnapPy to try to give a name\n",
    "    to the input manifold.\n",
    "    \"\"\"\n",
    "    \n",
    "    kind, name = None, None\n",
    "    \n",
    "    P = regina_tri.clone()\n",
    "    P.finiteToIdeal()\n",
    "    P.intelligentSimplify()\n",
    "    M = snappy.Manifold(P.isoSig())\n",
    "    M.simplify()\n",
    "    if appears_hyperbolic(M):\n",
    "        for i in range(100):\n",
    "            if M.solution_type() == 'all tetrahedra positively oriented':\n",
    "                break\n",
    "            M.randomize()\n",
    "        \n",
    "        if not M.verify_hyperbolicity(bits_prec=100):\n",
    "            raise RuntimeError('Cannot prove hyperbolicity for ' +\n",
    "                               M.triangulation_isosig())\n",
    "        kind = 'hyperbolic'\n",
    "        ids = M.identify()\n",
    "        if ids:\n",
    "            name = ids[0].name()\n",
    "    else:\n",
    "        match = standard_lookup(regina_tri)\n",
    "        if match is None:\n",
    "            Q = P.clone()\n",
    "            Q.idealToFinite()\n",
    "            Q.intelligentSimplify()\n",
    "            match = standard_lookup(Q)\n",
    "        if match is not None:\n",
    "            kind = match.__class__.__name__\n",
    "            name = str(match)\n",
    "        else:\n",
    "            name = P.isoSig()\n",
    "    return kind, name\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "def is_toroidal(regina_tri):\n",
    "    \"\"\"\n",
    "    Checks for essential tori and returns the pieces of the\n",
    "    associated partial JSJ decomposition.\n",
    "    \n",
    "    >>> T = to_regina('hLALAkbccfefgglpkusufk')  # m004(4,1)\n",
    "    >>> is_toroidal(T)[0]\n",
    "    True\n",
    "    >>> T = to_regina('hvLAQkcdfegfggjwajpmpw')  # m004(0,1)\n",
    "    >>> is_toroidal(T)[0]\n",
    "    True\n",
    "    >>> T = to_regina('nLLLLMLPQkcdgfihjlmmlkmlhshnrvaqtpsfnf')  # 5_2(10,1)\n",
    "    >>> T.isHaken()\n",
    "    True\n",
    "    >>> is_toroidal(T)[0]\n",
    "    False\n",
    "\n",
    "    Note: currently checks all fundamental normal tori; possibly\n",
    "    the theory lets one just check *vertex* normal tori.\n",
    "    \"\"\"\n",
    "    T = regina_tri\n",
    "    assert T.isZeroEfficient()\n",
    "    surfaces = regina.NNormalSurfaceList.enumerate(T,\n",
    "                          regina.NS_QUAD, regina.NS_FUNDAMENTAL)\n",
    "    for i in range(surfaces.size()):\n",
    "        S = surfaces.surface(i)\n",
    "        if S.eulerChar() == 0:\n",
    "            if not S.isOrientable():\n",
    "                S = S.doubleSurface()\n",
    "            assert S.isOrientable()\n",
    "            X = S.cutAlong()\n",
    "            X.intelligentSimplify()\n",
    "            X.splitIntoComponents()\n",
    "            pieces = list(children(X))\n",
    "            if all(not C.hasCompressingDisc() for C in pieces):\n",
    "                ids = [identify_with_torus_boundary(C) for C in pieces]\n",
    "                return (True, sorted(ids))\n",
    "                \n",
    "    return (False, None)\n",
    "\n",
    "\n",
    "def decompose_along_tori(regina_tri):\n",
    "    \"\"\"\n",
    "    First, finds all essential normal tori in the manifold associated\n",
    "    with fundamental normal surfaces.  Then takes a maximal disjoint\n",
    "    collection of these tori, namely the one with the fewest tori\n",
    "    involved, and cuts the manifold open along it.  It tries to\n",
    "    identify the pieces, removing any (torus x I) components. \n",
    "\n",
    "    Returns: (has essential torus, list of pieces)\n",
    "\n",
    "    Note: This may fail to be the true JSJ decomposition because there\n",
    "    could be (torus x I)'s in the list of pieces and it might well be\n",
    "    possible to amalgamate some of the pieces into a single SFS.\n",
    "    \"\"\"\n",
    "    \n",
    "    T = regina_tri\n",
    "    assert T.isZeroEfficient()\n",
    "    essential_tori = []\n",
    "    surfaces = regina.NNormalSurfaceList.enumerate(T,\n",
    "                          regina.NS_QUAD, regina.NS_FUNDAMENTAL)\n",
    "    for i in range(surfaces.size()):\n",
    "        S = surfaces.surface(i)\n",
    "        if S.eulerChar() == 0:\n",
    "            if not S.isOrientable():\n",
    "                S = S.doubleSurface()\n",
    "            assert S.isOrientable()\n",
    "            X = S.cutAlong()\n",
    "            X.intelligentSimplify()\n",
    "            X.splitIntoComponents()\n",
    "            pieces = list(children(X))\n",
    "            if all(not C.hasCompressingDisc() for C in pieces):\n",
    "                essential_tori.append(S)\n",
    "\n",
    "    if len(essential_tori) == 0:\n",
    "        return False, None\n",
    "    \n",
    "    D = nx.Graph()\n",
    "    for a, A in enumerate(essential_tori):\n",
    "        for b, B in enumerate(essential_tori):\n",
    "            if a < b:\n",
    "                if A.disjoint(B):\n",
    "                    D.add_edge(a, b)\n",
    "\n",
    "    cliques = list(nx.find_cliques(D))\n",
    "    if len(cliques) == 0:\n",
    "        clique = [0]\n",
    "    else:\n",
    "        clique = min(cliques, key=len)\n",
    "    clique = [essential_tori[c] for c in clique]\n",
    "    A = clique[0]\n",
    "    for B in clique[1:]:\n",
    "        A = haken_sum(A, B)\n",
    "\n",
    "    X = A.cutAlong()\n",
    "    X.intelligentSimplify()\n",
    "    X.splitIntoComponents()\n",
    "    ids = [identify_with_torus_boundary(C) for C in list(children(X))]\n",
    "    # Remove products\n",
    "    ids = [i for i in ids if i[1] not in ('SFS [A: (1,1)]', 'A x S1')]\n",
    "    return (True, sorted(ids))\n",
    "\n",
    "def regina_name(closed_snappy_manifold, trys=100):\n",
    "    \"\"\"\n",
    "    >>> regina_name('m004(1,0)')\n",
    "    'S3'\n",
    "    >>> regina_name('s006(-2, 1)')\n",
    "    'SFS [A: (5,1)] / [ 0,-1 | -1,0 ]'\n",
    "    >>> regina_name('m010(-1, 1)')\n",
    "    'L(3,1) # RP3'\n",
    "    >>> regina_name('m022(-1,1)')\n",
    "    'SFS [S2: (3,2) (3,2) (4,-3)]'\n",
    "    >>> regina_name('v0004(0, 1)')\n",
    "    'SFS [S2: (2,1) (4,1) (15,-13)]'\n",
    "    >>> regina_name('m305(1, 0)')\n",
    "    'L(3,1) # RP3'\n",
    "    \"\"\"\n",
    "    M = snappy.Manifold(closed_snappy_manifold)\n",
    "    isosigs = closed_isosigs(M, trys=trys, max_tets=25)\n",
    "    if len(isosigs) == 0:\n",
    "        return\n",
    "    T = to_regina(isosigs[0])\n",
    "    if T.isIrreducible():\n",
    "        if T.countTetrahedra() <= 11:\n",
    "            for i in range(3):\n",
    "                T.simplifyExhaustive(i)\n",
    "                name = census_lookup(T)\n",
    "                if name is not None:\n",
    "                    return name\n",
    "            \n",
    "        matches = [standard_lookup(to_regina(iso)) for iso in isosigs]\n",
    "        match = best_match(matches)\n",
    "        if match is not None:\n",
    "            return str(match)\n",
    "    else:\n",
    "        T.connectedSumDecomposition()\n",
    "        pieces = [regina_name(P) for P in children(T)]\n",
    "        if None not in pieces:\n",
    "            return ' # '.join(sorted(pieces))\n",
    "        \n",
    "def recognize_mfd(knot):\n",
    "    \"\"\"\n",
    "    Uses regina and snappy to recognize the name of its 0-filling.\n",
    "    \"\"\"\n",
    "    K=snappy.Manifold(knot)\n",
    "    K_reg=regina_name(K)\n",
    "    if K_reg is not None:\n",
    "        return K_reg  \n",
    "    else:\n",
    "        try:\n",
    "            K_reg=decompose_along_tori(to_regina(closed_isosigs(K)[0]))\n",
    "        except TypeError:\n",
    "            K_reg=None\n",
    "        if K_reg is not None and K_reg[0]==True:\n",
    "            return 'JSJ'+str(K_reg[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_crossing_knots=[]\n",
    "for c in range(5,10):\n",
    "    for K in snappy.HTLinkExteriors(knots_vs_links='knots',crossings=c):\n",
    "        name=K.identify()[1].name()\n",
    "        if name[0]=='K':\n",
    "            name=K.identify()[0].name()\n",
    "        if name!='5_2':\n",
    "            low_crossing_knots.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5_1', '6_3', '6_2', '6_1', '7_7', '7_6', '7_5', '7_2', '7_3', '7_4', '7_1', '8_14', '8_15', '8_10', '8_8', '8_12', '8_7', '8_13', '8_2', '8_11', '8_6', '8_1', '8_18', '8_5', '8_17', '8_16', '8_9', '8_4', '8_3', '8_20', '8_21', '8_19', '9_30', '9_22', '9_19', '9_25', '9_28', '9_32', '9_24', '9_8', '9_36', '9_15', '9_33', '9_27', '9_31', '9_17', '9_26', '9_23', '9_14', '9_37', '9_20', '9_11', '9_21', '9_12', '9_6', '9_18', '9_16', '9_7', '9_2', '9_34', '9_41', '9_38', '9_29', '9_39', '9_9', '9_13', '9_4', '9_5', '9_40', '9_3', '9_10', '9_35', '9_1', '9_44', '9_45', '9_43', '9_42', '9_46', '9_48', '9_47', '9_49']\n"
     ]
    }
   ],
   "source": [
    "print(low_crossing_knots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(low_crossing_knots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_same_0_surgeries=[] \n",
    "for name in low_crossing_knots:\n",
    "    listname=[]\n",
    "    try:\n",
    "        with open(name+'.csv', 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                listname.append(row)\n",
    "            listname=listname[1:]\n",
    "            low_cros=[]\n",
    "            high_cros=[]\n",
    "            for x in listname:\n",
    "                if int(x[1])<16:\n",
    "                    low_cros.append(x)\n",
    "                else:\n",
    "                    high_cros.append(x)\n",
    "            listname=[]\n",
    "            listname.append(low_cros)\n",
    "            listname.append(high_cros)\n",
    "            possible_same_0_surgeries.append(listname)\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(possible_same_0_surgeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we order these lists according to the cases that the 0-surgery of the low crossing not is hyperbolic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_1  is non-hyperbolic: SFS [S2: (2,1) (5,2) (10,-9)]\n",
      "---------\n",
      "6_3  is hyperbolic with volume: 4.059766425639?\n",
      "---------\n",
      "6_2  is hyperbolic with volume: 3.770829451108?\n",
      "---------\n",
      "6_1  is non-hyperbolic: SFS [A: (2,1)] / [ 0,1 | 1,-2 ]\n",
      "9_46  is non-hyperbolic: SFS [A: (2,1)] / [ -1,3 | 1,-2 ]\n",
      "---------\n",
      "7_7  is hyperbolic with volume: 6.332666642499?\n",
      "---------\n",
      "7_6  is hyperbolic with volume: 6.180274419374?\n",
      "---------\n",
      "7_5  is hyperbolic with volume: 5.987810443355?\n",
      "---------\n",
      "7_2  is non-hyperbolic: SFS [A: (3,2)] / [ 0,1 | 1,-1 ]\n",
      "---------\n",
      "7_3  is hyperbolic with volume: 4.218233644881?\n",
      "---------\n",
      "7_4  is non-hyperbolic: JSJ[('SFSpace', 'SFS [A: (2,1)]'), ('SFSpace', 'SFS [A: (2,3)]')]\n",
      "9_2  is non-hyperbolic: SFS [A: (4,3)] / [ 0,1 | 1,-1 ]\n",
      "---------\n",
      "7_1  is non-hyperbolic: SFS [S2: (2,1) (7,3) (14,-13)]\n",
      "---------\n",
      "8_14  is hyperbolic with volume: 8.19064265904?\n",
      "9_8  is hyperbolic with volume: 7.46191985006?\n",
      "---------\n",
      "8_15  is hyperbolic with volume: 9.34081310840?\n",
      "---------\n",
      "8_10  is hyperbolic with volume: 7.790015973497?\n",
      "---------\n",
      "8_8  is hyperbolic with volume: 6.38487304951?\n",
      "---------\n",
      "8_12  is hyperbolic with volume: 7.64659248195?\n",
      "---------\n",
      "8_7  is hyperbolic with volume: 6.111659915355?\n",
      "---------\n",
      "8_13  is hyperbolic with volume: 7.259889578204?\n",
      "---------\n",
      "8_2  is hyperbolic with volume: 4.703642059133?\n",
      "---------\n",
      "8_11  is hyperbolic with volume: 7.186378673021?\n",
      "---------\n",
      "8_6  is hyperbolic with volume: 6.33266664250?\n",
      "---------\n",
      "8_1  is non-hyperbolic: SFS [A: (3,1)] / [ 0,1 | 1,-2 ]\n",
      "---------\n",
      "8_18  is hyperbolic with volume: 11.14721822567?\n",
      "9_24  is hyperbolic with volume: 9.55789756113?\n",
      "---------\n",
      "8_5  is hyperbolic with volume: 6.73630906712?\n",
      "---------\n",
      "8_17  is hyperbolic with volume: 9.65085003623?\n",
      "---------\n",
      "8_16  is hyperbolic with volume: 9.78375114088?\n",
      "---------\n",
      "8_9  is hyperbolic with volume: 5.656244176662?\n",
      "---------\n",
      "8_4  is hyperbolic with volume: 5.036279417761?\n",
      "---------\n",
      "8_3  is non-hyperbolic: JSJ[('SFSpace', 'SFS [A: (2,1)]'), ('SFSpace', 'SFS [A: (2,3)]')]\n",
      "---------\n",
      "8_20  is non-hyperbolic: SFS [D: (2,1) (2,1)] U/m SFS [D: (3,1) (3,2)], m = [ 0,1 | 1,0 ]\n",
      "---------\n",
      "8_21  is hyperbolic with volume: 5.333489566898?\n",
      "---------\n",
      "8_19  is non-hyperbolic: SFS [S2: (3,2) (4,1) (12,-11)]\n",
      "---------\n",
      "9_30  is hyperbolic with volume: 10.826566603418?\n",
      "---------\n",
      "9_22  is hyperbolic with volume: 9.9156151811?\n",
      "---------\n",
      "9_19  is hyperbolic with volume: 8.73622201206?\n",
      "---------\n",
      "9_25  is hyperbolic with volume: 10.30085542973?\n",
      "---------\n",
      "9_28  is hyperbolic with volume: 10.75912390483?\n",
      "9_29  is hyperbolic with volume: 11.5051185131?\n",
      "---------\n",
      "9_32  is hyperbolic with volume: 12.21747301092?\n",
      "---------\n",
      "9_36  is hyperbolic with volume: 9.47634776991?\n",
      "---------\n",
      "9_15  is hyperbolic with volume: 8.70500592768?\n",
      "---------\n",
      "9_33  is hyperbolic with volume: 12.17184088633?\n",
      "---------\n",
      "9_27  is hyperbolic with volume: 9.84166805476?\n",
      "---------\n",
      "9_31  is hyperbolic with volume: 10.89696370271?\n",
      "---------\n",
      "9_17  is hyperbolic with volume: 8.82380876658?\n",
      "---------\n",
      "9_26  is hyperbolic with volume: 9.67554142603?\n",
      "---------\n",
      "9_23  is hyperbolic with volume: 9.87292878673?\n",
      "---------\n",
      "9_14  is hyperbolic with volume: 7.67266968706?\n",
      "---------\n",
      "9_37  is hyperbolic with volume: 9.65090397193?\n",
      "---------\n",
      "9_20  is hyperbolic with volume: 9.28781194115?\n",
      "---------\n",
      "9_11  is hyperbolic with volume: 7.91268477624?\n",
      "---------\n",
      "9_21  is hyperbolic with volume: 8.98396582336?\n",
      "---------\n",
      "9_12  is hyperbolic with volume: 7.63491871137?\n",
      "---------\n",
      "9_6  is hyperbolic with volume: 6.992345253451?\n",
      "---------\n",
      "9_18  is hyperbolic with volume: 9.30877751754?\n",
      "---------\n",
      "9_16  is hyperbolic with volume: 9.65931027945?\n",
      "---------\n",
      "9_7  is hyperbolic with volume: 7.40014483002?\n",
      "---------\n",
      "9_34  is hyperbolic with volume: 13.23190917916?\n",
      "---------\n",
      "9_41  is hyperbolic with volume: 10.85352845425?\n",
      "---------\n",
      "9_38  is hyperbolic with volume: 12.1931276781?\n",
      "---------\n",
      "9_39  is hyperbolic with volume: 11.65700086215?\n",
      "---------\n",
      "9_9  is hyperbolic with volume: 7.82241098274?\n",
      "---------\n",
      "9_13  is hyperbolic with volume: 8.44017037566?\n",
      "---------\n",
      "9_4  is hyperbolic with volume: 5.139539432543?\n",
      "---------\n",
      "9_5  is non-hyperbolic: JSJ[('SFSpace', 'SFS [A: (2,1)]'), ('SFSpace', 'SFS [A: (3,5)]')]\n",
      "---------\n",
      "9_40  is hyperbolic with volume: 14.17620475533?\n",
      "---------\n",
      "9_3  is hyperbolic with volume: 4.830922173980?\n",
      "---------\n",
      "9_10  is hyperbolic with volume: 8.00795830906?\n",
      "---------\n",
      "9_35  is non-hyperbolic: JSJ[('hyperbolic', 'm202')]\n",
      "---------\n",
      "9_1  is non-hyperbolic: SFS [S2: (2,1) (9,4) (18,-17)]\n",
      "---------\n",
      "9_44  is hyperbolic with volume: 5.333489566898?\n",
      "---------\n",
      "9_45  is hyperbolic with volume: 6.96575966545?\n",
      "---------\n",
      "9_43  is hyperbolic with volume: 5.62696404695?\n",
      "---------\n",
      "9_42  is hyperbolic with volume: 3.177293278601?\n",
      "---------\n",
      "9_48  is hyperbolic with volume: 8.34550173719?\n",
      "---------\n",
      "9_47  is hyperbolic with volume: 9.1217216111?\n",
      "---------\n",
      "9_49  is hyperbolic with volume: 8.80631003339?\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "possible_same_0_surgeries_hyp=[]\n",
    "possible_same_0_surgeries_non_hyp=[]\n",
    "for x in possible_same_0_surgeries:\n",
    "    for name in x[0]:\n",
    "        K=snappy.Manifold(name[0])\n",
    "        K.dehn_fill((0,1))\n",
    "        vol=better_volume(K,try_hard=True)\n",
    "        if vol==0:\n",
    "            rec=recognize_mfd(K)\n",
    "            if rec==None:\n",
    "                K.randomize()\n",
    "                rec=recognize_mfd(K)\n",
    "            print(name[0],' is non-hyperbolic:',rec)\n",
    "            if x not in possible_same_0_surgeries_non_hyp:\n",
    "                possible_same_0_surgeries_non_hyp.append(x)\n",
    "        if vol!=0:\n",
    "            print(name[0],' is hyperbolic with volume:',vol)\n",
    "            if x not in possible_same_0_surgeries_hyp:\n",
    "                possible_same_0_surgeries_hyp.append(x)\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(possible_same_0_surgeries_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(possible_same_0_surgeries_non_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the 12 non-hyperbolic examples. Here we will first remove all hyperbolic 0-fillings with the same Alexander polynomial. In a second step we will use the number of coverings to further distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible equal surgeries: [[['5_1', '5', '']], [['19nh_000000149', '19', 'lHpMrNiBkQgaODFsJec']]]\n",
      "Possible equal surgeries: [[['6_1', '6', ''], ['9_46', '9', '']], [['17nh_0000012', '17', 'nmlkjiOqpedcbaGFh'], ['17nh_0000138', '17', 'djFaGnCQlObPheKIM'], ['18nh_00000334', '18', 'nOjMhbRcPefADqKIlG'], ['18nh_00000335', '18', 'gJehqNaMdRoBIPkFcL'], ['18nh_00000400', '18', 'oPjMiNbRecgAFDqKlH'], ['18nh_00000707', '18', 'lHoEMqiBkPgaNDrJfc'], ['19ns_018', '19', 'mQkGONDJrLcHaPEFSiB'], ['19ns_183', '19', 'qKeIaMcpRlBnFjDsgOH'], ['19ns_186', '19', 'qDjFnBlpRcMaIeKsgOH'], ['19nh_000000010', '19', 'ponmlkjQsrfedcbaHGi'], ['19nh_000000156', '19', 'nHqkjOiBmedRgaPFsLc'], ['19nh_000000333', '19', 'pKsIlOqCmGAfRhEbJdN'], ['19nh_000000334', '19', 'fqKrOsPlgEDBhaJIMcn']]]\n",
      "Possible equal surgeries: [[['7_2', '7', '']], [['16nh_0000443', '16', 'eIGMaCkPOlfnDjBH'], ['17nh_0000009', '17', 'dNjaFIqmElochPkBg']]]\n",
      "Possible equal surgeries: [[['7_4', '7', ''], ['9_2', '9', '']], [['17nh_0000005', '17', 'nDkFmBjiqgcOeaPLh'], ['18ns_24', '18', 'lPjFNMIqKcGaOEDRhB'], ['18nh_00000598', '18', 'mHKGrOJLnFCBqPiEad']]]\n",
      "Possible equal surgeries: [[['7_1', '7', '']], [['18nh_00000002', '18', 'mPLKrJINGFECqOHABd']]]\n",
      "Possible equal surgeries: [[['8_1', '8', '']], []]\n",
      "Possible equal surgeries: [[['8_3', '8', '']], [['16ns_03', '16', 'cJoELPiNkBgMDHaF'], ['16nh_0000296', '16', 'kGnELhBjOfaMDpIc'], ['17ns_06', '17', 'kOiLMHpJcFaNEDQgB']]]\n",
      "Possible equal surgeries: [[['8_20', '8', '']], [['16nh_0000011', '16', 'gNMLKJpIOFEDCBHa']]]\n",
      "Possible equal surgeries: [[['8_19', '8', '']], [['16nh_0000001', '16', 'kNJIpHLFECoMGABd']]]\n",
      "Possible equal surgeries: [[['9_5', '9', '']], [['16ns_06', '16', 'kNiMLHoJcFaEDPgB']]]\n",
      "Possible equal surgeries: [[['9_35', '9', '']], []]\n",
      "Possible equal surgeries: [[['9_1', '9', '']], []]\n",
      "Time taken: 1.9723915865686206 hours \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "possible_same_0_surgeries_both_prop_non_hyp=[]\n",
    "\n",
    "for x in possible_same_0_surgeries_non_hyp:\n",
    "    unclear=[]\n",
    "    for [knot,cros,DT] in x[1]:\n",
    "        K=snappy.Link(change_notation(DT)).exterior()\n",
    "        K.dehn_fill((0,1))\n",
    "        vol=better_volume(K)\n",
    "        if vol==0:\n",
    "            unclear.append([knot,cros,DT])\n",
    "    possible_same_0_surgeries_both_prop_non_hyp.append([x[0],unclear]) \n",
    "    print('Possible equal surgeries:',[x[0],unclear])\n",
    "print('Time taken: %s hours ' % ((time.time() - start_time)/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue to work with this reduced list. First we try to show again that the remaining manifolds are hyperbolic and if this does not work we try to distinguish using the numbers of branched covers of degree n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5_1', '5', '']]\n",
      "19nh_000000149 6.47459498758?\n",
      "----------------\n",
      "[['6_1', '6', ''], ['9_46', '9', '']]\n",
      "17nh_0000012 0\n",
      "17nh_0000138 0\n",
      "18nh_00000334 0\n",
      "18nh_00000335 0\n",
      "18nh_00000400 7.90216370908?\n",
      "18nh_00000707 0\n",
      "19ns_018 0\n",
      "19ns_183 0\n",
      "19ns_186 0\n",
      "19nh_000000010 0\n",
      "19nh_000000156 0\n",
      "19nh_000000333 0\n",
      "19nh_000000334 0\n",
      "----------------\n",
      "[['7_2', '7', '']]\n",
      "16nh_0000443 0\n",
      "17nh_0000009 0\n",
      "----------------\n",
      "[['7_4', '7', ''], ['9_2', '9', '']]\n",
      "17nh_0000005 0\n",
      "18ns_24 0\n",
      "18nh_00000598 0\n",
      "----------------\n",
      "[['7_1', '7', '']]\n",
      "18nh_00000002 0\n",
      "----------------\n",
      "[['8_1', '8', '']]\n",
      "----------------\n",
      "[['8_3', '8', '']]\n",
      "16ns_03 0\n",
      "16nh_0000296 0\n",
      "17ns_06 0\n",
      "----------------\n",
      "[['8_20', '8', '']]\n",
      "16nh_0000011 0\n",
      "----------------\n",
      "[['8_19', '8', '']]\n",
      "16nh_0000001 0\n",
      "----------------\n",
      "[['9_5', '9', '']]\n",
      "16ns_06 0\n",
      "----------------\n",
      "[['9_35', '9', '']]\n",
      "----------------\n",
      "[['9_1', '9', '']]\n",
      "----------------\n",
      "Time taken: 1.325797704855601 minutes \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "still_unclear=[]\n",
    "for x in possible_same_0_surgeries_both_prop_non_hyp:\n",
    "    print(x[0])\n",
    "    unclear=[]\n",
    "    for [knot,cros,DT] in x[1]:\n",
    "        K=snappy.Link(change_notation(DT)).exterior()\n",
    "        K.dehn_fill((0,1))\n",
    "        vol=better_volume(K,try_hard=True)\n",
    "        print(knot,vol)\n",
    "        if vol==0:\n",
    "            unclear.append([knot,cros,DT])\n",
    "    if len(unclear)>0:\n",
    "        still_unclear.append([x[0],unclear])\n",
    "    print('----------------')\n",
    "print('Time taken: %s minutes ' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(still_unclear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['6_1', '6', ''], ['9_46', '9', '']],\n",
       "  [['17nh_0000012', '17', 'nmlkjiOqpedcbaGFh'],\n",
       "   ['17nh_0000138', '17', 'djFaGnCQlObPheKIM'],\n",
       "   ['18nh_00000334', '18', 'nOjMhbRcPefADqKIlG'],\n",
       "   ['18nh_00000335', '18', 'gJehqNaMdRoBIPkFcL'],\n",
       "   ['18nh_00000707', '18', 'lHoEMqiBkPgaNDrJfc'],\n",
       "   ['19ns_018', '19', 'mQkGONDJrLcHaPEFSiB'],\n",
       "   ['19ns_183', '19', 'qKeIaMcpRlBnFjDsgOH'],\n",
       "   ['19ns_186', '19', 'qDjFnBlpRcMaIeKsgOH'],\n",
       "   ['19nh_000000010', '19', 'ponmlkjQsrfedcbaHGi'],\n",
       "   ['19nh_000000156', '19', 'nHqkjOiBmedRgaPFsLc'],\n",
       "   ['19nh_000000333', '19', 'pKsIlOqCmGAfRhEbJdN'],\n",
       "   ['19nh_000000334', '19', 'fqKrOsPlgEDBhaJIMcn']]],\n",
       " [[['7_2', '7', '']],\n",
       "  [['16nh_0000443', '16', 'eIGMaCkPOlfnDjBH'],\n",
       "   ['17nh_0000009', '17', 'dNjaFIqmElochPkBg']]],\n",
       " [[['7_4', '7', ''], ['9_2', '9', '']],\n",
       "  [['17nh_0000005', '17', 'nDkFmBjiqgcOeaPLh'],\n",
       "   ['18ns_24', '18', 'lPjFNMIqKcGaOEDRhB'],\n",
       "   ['18nh_00000598', '18', 'mHKGrOJLnFCBqPiEad']]],\n",
       " [[['7_1', '7', '']], [['18nh_00000002', '18', 'mPLKrJINGFECqOHABd']]],\n",
       " [[['8_3', '8', '']],\n",
       "  [['16ns_03', '16', 'cJoELPiNkBgMDHaF'],\n",
       "   ['16nh_0000296', '16', 'kGnELhBjOfaMDpIc'],\n",
       "   ['17ns_06', '17', 'kOiLMHpJcFaNEDQgB']]],\n",
       " [[['8_20', '8', '']], [['16nh_0000011', '16', 'gNMLKJpIOFEDCBHa']]],\n",
       " [[['8_19', '8', '']], [['16nh_0000001', '16', 'kNJIpHLFECoMGABd']]],\n",
       " [[['9_5', '9', '']], [['16ns_06', '16', 'kNiMLHoJcFaEDPgB']]]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "still_unclear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgroups_of_order_n(M,n):\n",
    "    '''\n",
    "    Returns the number of subgroups of order n of the fundamental group of M.\n",
    "    Warning: Works only fast for small n.\n",
    "    '''\n",
    "    return len(snappy.Manifold(M).covers(n))\n",
    "\n",
    "def subgroups_up_to_order_k(M,k):\n",
    "    '''\n",
    "    Returns the vector containing the number of subgroups of order n of the fundamental group of M for n=2,...,k.\n",
    "    Warning: Works only fast for small k.\n",
    "    '''\n",
    "    vector=[]\n",
    "    for n in range(2,k+1):\n",
    "        vector.append(subgroups_of_order_n(M,n))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['6_1', '6', '', [1, 2, 1, 3, 3]], ['9_46', '9', '', [1, 5, 4, 6, 24]]]\n",
      "6_1 Number of subgroups: [1, 2, 1, 3, 3]\n",
      "17nh_0000012 Number of subgroups: [1, 2, 1, 3]\n",
      "17nh_0000138 Number of subgroups: [1, 2, 4, 10]\n",
      "18nh_00000334 Number of subgroups: [1, 5, 4, 6]\n",
      "18nh_00000335 Number of subgroups: [1, 5, 4, 6]\n",
      "18nh_00000707 Number of subgroups: [1, 2, 1, 8]\n",
      "19ns_018 Number of subgroups: [1, 2, 4, 6]\n",
      "19ns_183 Number of subgroups: [1, 2, 1, 13]\n",
      "19ns_186 Number of subgroups: [1, 2, 1, 33]\n",
      "19nh_000000010 Number of subgroups: [1, 2, 1, 3]\n",
      "19nh_000000156 Number of subgroups: [1, 2, 4, 11]\n",
      "19nh_000000333 Number of subgroups: [1, 2, 1, 3]\n",
      "19nh_000000334 Number of subgroups: [1, 2, 1, 3]\n",
      "9_46 Number of subgroups: [1, 5, 4, 6, 24]\n",
      "17nh_0000012 Number of subgroups: [1, 2, 1, 3]\n",
      "17nh_0000138 Number of subgroups: [1, 2, 4, 10]\n",
      "18nh_00000334 Number of subgroups: [1, 5, 4, 6]\n",
      "18nh_00000335 Number of subgroups: [1, 5, 4, 6]\n",
      "18nh_00000707 Number of subgroups: [1, 2, 1, 8]\n",
      "19ns_018 Number of subgroups: [1, 2, 4, 6]\n",
      "19ns_183 Number of subgroups: [1, 2, 1, 13]\n",
      "19ns_186 Number of subgroups: [1, 2, 1, 33]\n",
      "19nh_000000010 Number of subgroups: [1, 2, 1, 3]\n",
      "19nh_000000156 Number of subgroups: [1, 2, 4, 11]\n",
      "19nh_000000333 Number of subgroups: [1, 2, 1, 3]\n",
      "19nh_000000334 Number of subgroups: [1, 2, 1, 3]\n",
      "----------------\n",
      "[['7_2', '7', '', [1, 1, 2, 3, 3]]]\n",
      "7_2 Number of subgroups: [1, 1, 2, 3, 3]\n",
      "16nh_0000443 Number of subgroups: [1, 1, 2, 3]\n",
      "17nh_0000009 Number of subgroups: [1, 1, 2, 13]\n",
      "----------------\n",
      "[['7_4', '7', '', [1, 2, 1, 2, 11]], ['9_2', '9', '', [1, 2, 1, 2, 3]]]\n",
      "7_4 Number of subgroups: [1, 2, 1, 2, 11]\n",
      "17nh_0000005 Number of subgroups: [1, 2, 4, 5]\n",
      "18ns_24 Number of subgroups: [1, 2, 4, 5]\n",
      "18nh_00000598 Number of subgroups: [1, 2, 1, 2]\n",
      "9_2 Number of subgroups: [1, 2, 1, 2, 3]\n",
      "17nh_0000005 Number of subgroups: [1, 2, 4, 5]\n",
      "18ns_24 Number of subgroups: [1, 2, 4, 5]\n",
      "18nh_00000598 Number of subgroups: [1, 2, 1, 2]\n",
      "----------------\n",
      "[['7_1', '7', '', [1, 1, 1, 1, 1]]]\n",
      "7_1 Number of subgroups: [1, 1, 1, 1, 1]\n",
      "18nh_00000002 Number of subgroups: [1, 1, 1, 1]\n",
      "----------------\n",
      "[['8_3', '8', '', [1, 1, 1, 1, 11]]]\n",
      "8_3 Number of subgroups: [1, 1, 1, 1, 11]\n",
      "16ns_03 Number of subgroups: [1, 1, 1, 1]\n",
      "16nh_0000296 Number of subgroups: [1, 1, 1, 1]\n",
      "17ns_06 Number of subgroups: [1, 1, 1, 11]\n",
      "----------------\n",
      "[['8_20', '8', '', [1, 2, 5, 2, 12]]]\n",
      "8_20 Number of subgroups: [1, 2, 5, 2, 12]\n",
      "16nh_0000011 Number of subgroups: [1, 2, 5, 7]\n",
      "----------------\n",
      "[['8_19', '8', '', [1, 2, 5, 4, 20]]]\n",
      "8_19 Number of subgroups: [1, 2, 5, 4, 20]\n",
      "16nh_0000001 Number of subgroups: [1, 2, 5, 4]\n",
      "----------------\n",
      "[['9_5', '9', '', [1, 1, 1, 1, 1]]]\n",
      "9_5 Number of subgroups: [1, 1, 1, 1, 1]\n",
      "16ns_06 Number of subgroups: [1, 1, 1, 6]\n",
      "----------------\n",
      "Time taken: 0.04368443091710408 minutes \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "might_be_the_same=[]\n",
    "for x in still_unclear:\n",
    "    print(x[0])\n",
    "    for y in x[0]:\n",
    "        unclear=[]\n",
    "        K=snappy.Manifold(y[0])\n",
    "        K.dehn_fill((0,1))\n",
    "        vec0=subgroups_up_to_order_k(K,6)\n",
    "        print(y[0],'Number of subgroups:',vec0)\n",
    "        for [knot,cros,DT] in x[1]:\n",
    "            K=snappy.Link(change_notation(DT)).exterior()\n",
    "            K.dehn_fill((0,1))\n",
    "            vec=subgroups_up_to_order_k(K,5)\n",
    "            print(knot,'Number of subgroups:',vec)\n",
    "            if vec==vec0[:-1]:\n",
    "                unclear.append([knot,cros,DT,vec])\n",
    "        if len(unclear)>1:\n",
    "            might_be_the_same.append([[y[0],y[1],y[2],vec0],unclear])\n",
    "                \n",
    "    \n",
    "    print('----------------')\n",
    "print('Time taken: %s minutes ' % ((time.time() - start_time)/60))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['6_1', '6', '', [1, 2, 1, 3, 3]],\n",
       "  [['17nh_0000012', '17', 'nmlkjiOqpedcbaGFh', [1, 2, 1, 3]],\n",
       "   ['19nh_000000010', '19', 'ponmlkjQsrfedcbaHGi', [1, 2, 1, 3]],\n",
       "   ['19nh_000000333', '19', 'pKsIlOqCmGAfRhEbJdN', [1, 2, 1, 3]],\n",
       "   ['19nh_000000334', '19', 'fqKrOsPlgEDBhaJIMcn', [1, 2, 1, 3]]]],\n",
       " [['9_46', '9', '', [1, 5, 4, 6, 24]],\n",
       "  [['18nh_00000334', '18', 'nOjMhbRcPefADqKIlG', [1, 5, 4, 6]],\n",
       "   ['18nh_00000335', '18', 'gJehqNaMdRoBIPkFcL', [1, 5, 4, 6]]]],\n",
       " [['8_3', '8', '', [1, 1, 1, 1, 11]],\n",
       "  [['16ns_03', '16', 'cJoELPiNkBgMDHaF', [1, 1, 1, 1]],\n",
       "   ['16nh_0000296', '16', 'kGnELhBjOfaMDpIc', [1, 1, 1, 1]]]]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "might_be_the_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6_1', '6', '', [1, 2, 1, 3, 3]]\n",
      "17nh_0000012 nmlkjiOqpedcbaGFh Number of subgroups: [1, 2, 1, 3, 3]\n",
      "19nh_000000010 ponmlkjQsrfedcbaHGi Number of subgroups: [1, 2, 1, 3, 3]\n",
      "19nh_000000333 pKsIlOqCmGAfRhEbJdN Number of subgroups: [1, 2, 1, 3, 21]\n",
      "19nh_000000334 fqKrOsPlgEDBhaJIMcn Number of subgroups: [1, 2, 1, 3, 21]\n",
      "----------------\n",
      "['9_46', '9', '', [1, 5, 4, 6, 24]]\n",
      "18nh_00000334 nOjMhbRcPefADqKIlG Number of subgroups: [1, 5, 4, 6, 24]\n",
      "18nh_00000335 gJehqNaMdRoBIPkFcL Number of subgroups: [1, 5, 4, 6, 24]\n",
      "----------------\n",
      "['8_3', '8', '', [1, 1, 1, 1, 11]]\n",
      "16ns_03 cJoELPiNkBgMDHaF Number of subgroups: [1, 1, 1, 1, 21]\n",
      "16nh_0000296 kGnELhBjOfaMDpIc Number of subgroups: [1, 1, 1, 1, 21]\n",
      "----------------\n",
      "Time taken: 0.028177130222320556 minutes \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for x in might_be_the_same:\n",
    "    print(x[0])\n",
    "    for [knot,cros,DT,v] in x[1]:\n",
    "            K=snappy.Link(change_notation(DT)).exterior()\n",
    "            K.dehn_fill((0,1))\n",
    "            vec=subgroups_up_to_order_k(K,6)\n",
    "            print(knot,DT,'Number of subgroups:',vec)\n",
    "    print('----------------')\n",
    "print('Time taken: %s minutes ' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=snappy.Manifold('6_1')\n",
    "K.dehn_fill((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgroups_of_order_n(K,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=snappy.Link(change_notation('nmlkjiOqpedcbaGFh')).exterior()\n",
    "K.dehn_fill((0,1))\n",
    "subgroups_of_order_n(K,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=snappy.Link(change_notation('ponmlkjQsrfedcbaHGi')).exterior()\n",
    "K.dehn_fill((0,1))\n",
    "subgroups_of_order_n(K,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=snappy.Manifold('9_46')\n",
    "K.dehn_fill((0,1))\n",
    "subgroups_of_order_n(K,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=snappy.Link(change_notation('nOjMhbRcPefADqKIlG')).exterior()\n",
    "K.dehn_fill((0,1))\n",
    "subgroups_of_order_n(K,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=snappy.Link(change_notation('gJehqNaMdRoBIPkFcL')).exterior()\n",
    "K.dehn_fill((0,1))\n",
    "subgroups_of_order_n(K,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining examples this seems not to help. So here we use regina to recognize the manifold and check if we get a match.\n",
    "\n",
    "We recall that:\n",
    "\n",
    "6_1 (0,1) is non-hyperbolic: SFS [A: (2,1)] / [ 0,1 | 1,-2 ]\n",
    "\n",
    "9_46 (0,1) is non-hyperbolic: SFS [A: (2,1)] / [ -1,3 | 1,-2 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SFS [A: (2,1)] / [ 2,11 | 1,5 ]'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=snappy.Link(change_notation('nmlkjiOqpedcbaGFh')).exterior()\n",
    "K.dehn_fill((0,1))\n",
    "recognize_mfd(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SFS [A: (2,1)] / [ 2,13 | 1,6 ]'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=snappy.Link(change_notation('ponmlkjQsrfedcbaHGi')).exterior()\n",
    "K.dehn_fill((0,1))\n",
    "recognize_mfd(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"JSJ[('hyperbolic', 's785')]\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=snappy.Link(change_notation('nOjMhbRcPefADqKIlG')).exterior()\n",
    "K.dehn_fill((0,1))\n",
    "recognize_mfd(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"JSJ[('hyperbolic', 's780')]\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=snappy.Link(change_notation('gJehqNaMdRoBIPkFcL')).exterior()\n",
    "K.dehn_fill((0,1))\n",
    "recognize_mfd(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seifert fibered space over the annulus we can distinguish via the traces of their gluing maps. The other manifolds are right away not diffoemorphic since they have different JSJ decompositions. This finishes the classification in the non-hyperbolic case.\n",
    "\n",
    "We continue with the hyperbolic 0-fillings. Here we use the verified volumes to distinguish. First we create lists of manifolds where the volume overlaps. If the volume cannot be comuted we try to verify the manifold to be non-hyperbolic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_3 Volume: 4.059766425639?\n",
      "----------------\n",
      "6_2 Volume: 3.770829451108?\n",
      "----------------\n",
      "7_7 Volume: 6.332666642499?\n",
      "----------------\n",
      "7_6 Volume: 6.180274419374?\n",
      "----------------\n",
      "7_5 Volume: 5.987810443355?\n",
      "----------------\n",
      "7_3 Volume: 4.218233644881?\n",
      "----------------\n",
      "8_14 Volume: 8.19064265904?\n",
      "9_8 Volume: 7.46191985006?\n",
      "----------------\n",
      "8_15 Volume: 9.34081310840?\n",
      "----------------\n",
      "8_10 Volume: 7.790015973497?\n",
      "----------------\n",
      "8_8 Volume: 6.38487304951?\n",
      "----------------\n",
      "8_12 Volume: 7.64659248195?\n",
      "----------------\n",
      "8_7 Volume: 6.111659915355?\n",
      "----------------\n",
      "8_13 Volume: 7.259889578204?\n",
      "----------------\n",
      "8_2 Volume: 4.703642059133?\n",
      "----------------\n",
      "8_11 Volume: 7.186378673021?\n",
      "----------------\n",
      "8_6 Volume: 6.33266664250?\n",
      "----------------\n",
      "8_18 Volume: 11.14721822567?\n",
      "9_24 Volume: 9.55789756113?\n",
      "----------------\n",
      "8_5 Volume: 6.73630906712?\n",
      "----------------\n",
      "8_17 Volume: 9.65085003623?\n",
      "----------------\n",
      "8_16 Volume: 9.78375114088?\n",
      "----------------\n",
      "8_9 Volume: 5.65624417667?\n",
      "----------------\n",
      "8_4 Volume: 5.036279417761?\n",
      "----------------\n",
      "8_21 Volume: 5.333489566898?\n",
      "----------------\n",
      "9_30 Volume: 10.826566603418?\n",
      "----------------\n",
      "9_22 Volume: 9.9156151811?\n",
      "----------------\n",
      "9_19 Volume: 8.73622201206?\n",
      "----------------\n",
      "9_25 Volume: 10.30085542973?\n",
      "----------------\n",
      "9_28 Volume: 10.75912390483?\n",
      "9_29 Volume: 11.5051185131?\n",
      "----------------\n",
      "9_32 Volume: 12.21747301092?\n",
      "----------------\n",
      "9_36 Volume: 9.47634776991?\n",
      "----------------\n",
      "9_15 Volume: 8.70500592768?\n",
      "----------------\n",
      "9_33 Volume: 12.17184088633?\n",
      "----------------\n",
      "9_27 Volume: 9.84166805476?\n",
      "----------------\n",
      "9_31 Volume: 10.89696370271?\n",
      "----------------\n",
      "9_17 Volume: 8.82380876658?\n",
      "----------------\n",
      "9_26 Volume: 9.67554142603?\n",
      "----------------\n",
      "9_23 Volume: 9.87292878673?\n",
      "----------------\n",
      "9_14 Volume: 7.67266968706?\n",
      "----------------\n",
      "9_37 Volume: 9.65090397193?\n",
      "----------------\n",
      "9_20 Volume: 9.28781194115?\n",
      "----------------\n",
      "9_11 Volume: 7.91268477624?\n",
      "----------------\n",
      "9_21 Volume: 8.98396582336?\n",
      "----------------\n",
      "9_12 Volume: 7.63491871137?\n",
      "----------------\n",
      "9_6 Volume: 6.992345253451?\n",
      "----------------\n",
      "9_18 Volume: 9.30877751754?\n",
      "----------------\n",
      "9_16 Volume: 9.65931027945?\n",
      "----------------\n",
      "9_7 Volume: 7.40014483002?\n",
      "----------------\n",
      "9_34 Volume: 13.23190917916?\n",
      "----------------\n",
      "9_41 Volume: 10.85352845425?\n",
      "----------------\n",
      "9_38 Volume: 12.1931276781?\n",
      "----------------\n",
      "9_39 Volume: 11.65700086215?\n",
      "----------------\n",
      "9_9 Volume: 7.82241098274?\n",
      "----------------\n",
      "9_13 Volume: 8.44017037566?\n",
      "----------------\n",
      "9_4 Volume: 5.139539432543?\n",
      "----------------\n",
      "9_40 Volume: 14.17620475533?\n",
      "----------------\n",
      "9_3 Volume: 4.830922173980?\n",
      "----------------\n",
      "9_10 Volume: 8.00795830906?\n",
      "----------------\n",
      "9_44 Volume: 5.333489566898?\n",
      "----------------\n",
      "9_45 Volume: 6.96575966545?\n",
      "----------------\n",
      "9_43 Volume: 5.62696404695?\n",
      "----------------\n",
      "9_42 Volume: 3.177293278601?\n",
      "----------------\n",
      "9_48 Volume: 8.34550173719?\n",
      "----------------\n",
      "9_47 Volume: 9.12172161102?\n",
      "----------------\n",
      "9_49 Volume: 8.80631003339?\n",
      "----------------\n",
      "Time taken: 190.70469505786895 minutes \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pairs_with_same_or_unclear_volume=[]\n",
    "for x in possible_same_0_surgeries_hyp:\n",
    "    for y in x[0]:\n",
    "        unclear=[]\n",
    "        K=snappy.Manifold(y[0])\n",
    "        K.dehn_fill((0,1))\n",
    "        vol0=better_volume(K,try_hard=True)\n",
    "        print(y[0],'Volume:',vol0)\n",
    "        for [knot,cros,DT] in x[1]:\n",
    "            K=snappy.Link(change_notation(DT)).exterior()\n",
    "            K.dehn_fill((0,1))\n",
    "            vol=better_volume(K,try_hard=True)\n",
    "            if vol==0:\n",
    "                name=recognize_mfd(K)\n",
    "                if name is None:\n",
    "                    print('We could not determine the hyperbolicity of:',knot,DT)\n",
    "                    unclear.append([knot,cros,DT])\n",
    "            if vol!=0:\n",
    "                if vol.overlaps(vol0):\n",
    "                    unclear.append([knot,cros,DT])\n",
    "        if len(unclear)>1:\n",
    "            pairs_with_same_or_unclear_volume.append([[y[0],y[1],y[2]],unclear])\n",
    "                \n",
    "    \n",
    "    print('----------------')\n",
    "print('Time taken: %s minutes ' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_with_same_or_unclear_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proves that all other 0-fillings of manifolds with the same Alexander polynomial are hyperbolic with different volume. This is somehow expected since these manifolds will usually be hyperbolic with larger volume.\n",
    "\n",
    "This proves that there is no pair of prime knots (K,K') such that K has at most 9 corrsings and K' has between (including) 16 and 19 crossings with K(0,1)=K'(0,1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.0",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
